{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process was adopted from [Rathje et al](https://www.pnas.org/doi/10.1073/pnas.2308950121)., 2021's open source code. We originally coded this process up in R and used R train GPT on our data. Here is the code translated in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# install packages\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import irr\n",
    "import knitr\n",
    "import kableExtra\n",
    "import dplyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load API\n",
    "\n",
    "my_API = INSERT GPT API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# write hey chat gpt function to prompt GPT to interpret and then categorize the textual data\n",
    "\n",
    "def hey_chatGPT(answer_my_question):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {my_API}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": answer_my_question\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load in training dataset\n",
    "\n",
    "data = pd.read_csv(INSERT TRAINING DATA)\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create new columns for GPT's classifications for each code\n",
    "\n",
    "data['emojis_gpt'] = None\n",
    "data['pos_ux_gpt'] = None\n",
    "data['neg_ux_gpt'] = None\n",
    "data['pos_ppl_gpt'] = None\n",
    "data['neg_ppl_gpt'] = None\n",
    "data['price_gpt'] = None\n",
    "data['fraud_gpt'] = None\n",
    "data['ban_gpt'] = None\n",
    "data['good_match_gpt'] = None\n",
    "data['bad_match_gpt'] = None\n",
    "data['pos_sent_gpt'] = None\n",
    "data['neg_sent_gpt'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# insert prompt engineering to help GPT understand each code\n",
    "\n",
    "emojis_question = \"Read the following dating app review. Does the review use emojis? Answer only with a number: 1 if the review uses emojis, 0 if the review does not use emojis. Here is the review:\"\n",
    "\n",
    "pos_ux_question = \"Read the following dating app review. Does this review praise with the app for working well and feeling genuine? Does the review discuss having a positive user experience (ex. positive experiences with app interface, easy to create a use an account, generally easy and enjoyable to use, enjoying the app features, etc) or any other positive compliments about the UX? A response that simply says the app is good, without expanding on some detail about a good user experience, is not enough to qualify for this positive UX code. Answer only with a number: 1 if the review discusses a the app working well or providing a positive user experience, 0 if the review does not discuss the app working well or providing a positive user experience. Here is the review:\"\n",
    "\n",
    "neg_ux_question = \"Read the following dating app review. Does this review discuss any problems with the user experience (ex. ndifficulty logging in and setting up an account, app being slow, having to wait on a waitlist to join the app, unhelpful customer support, running out of likes, egative experiences with the interface, having problems with the app features, etc), or any other negative critiques of the UX? To qualify for the negative UX code, the response needs to name how app feature(s), such as a slow waitlist, led to the overall negative UX. Simply stating complaints about the app, such as being banned or having to pay too much for the account, without naming app feature(s), is not enough to qualify for the negative UX code. Answer only with a number: 1 if the review discusses a negative user experience, 0 if the review does not discuss a negative user experience. Here is the review:\"\n",
    "\n",
    "pos_ppl_question = \"Read the following dating app review. Does this review discuss positive social interactions or relationships with other users on the app (ex. making friends, finding love, having great connections and conversations, etc)? Answer only with a number: 1 if the review discusses positive social interactions with other app users, 0 if the review does not discuss positive social interactions with other app users. Here is the review:\"\n",
    "\n",
    "neg_ppl_question = \"Read the following dating app review. Does this review discuss negative social interactions or relationships with other users on the app (ex. getting ghosted, being harrassed, not being able to connect with or enjoy conversations, etc)? Answer only with a number: 1 if the review discusses negative social interactions with other app users, 0 if the review does not discuss negative social interactions with other app users. Here is the review:\"\n",
    "\n",
    "price_question = \"Read the following dating app review. Does the review discuss any complaints or dissatisfaction with the monetization of the app (ex. the app charges money for too many features, the app's fees and prices are high, the app is wrongfully monetizing love, calling the monetization a scam, etc)? Answer only with a number: 1 if the review discusses negative views about the pricing of the app, 0 if the review does not discuss negative views about the pricing of the app. Here is the review:\"\n",
    "\n",
    "fraud_question = \"Read the following dating app review. Does the review discuss encountering fraudulent accounts on the app? To qualify for the fraud code, the review must explicitly be discussing encountering fake dating profiles or inaccurate information about other dating app users. Other discussions of fraud, such as the app being a scam and not being worth the money, does not qualify for this code. Answer only with a number: 1 if the review discusses encountering fraud on the app, 0 if the review does not discuss encountering fraud on the app.\"\n",
    "\n",
    "ban_question = \"Read the following dating app review. Does the review discuss the reviewr's accout being banned, disabled, shut down, or otherwise closed down so the reviewer cannot access the account? Discussions of the reviewer wanting to cancel their account but being unable to does not qualify for the ban code. Answer only with a number: 1 if the review discusses the reviwer's account being banned, and 0 if the review does not discuss the reviewer's account being banned.\"\n",
    "\n",
    "good_match_question = \"Read the following dating app review. Does the review discuss the reviewer being happy about the profiles they see on the app (ex. saying they could find love or make friends with the people they see on the app)? Are the recommended matches or other users' dating app profiles that the reviewer encounters on the app as being good and reflective of the types of matches the reviewer was looking for (ex. the app filters showed the reviewer accounts that they were interested in, the potential matches being on the same page as the reviewer, etc)? Simply stating 'good' in the review without further detail does not qualify for this 'good match' code. Answer only with a number: 1 if the review discusses the app providing a good pool of potential matches for the reviewer, 0 if the review does not discuss the app providing a good pool of potential matches for the reviewer.\"\n",
    "\n",
    "bad_match_question = \"Read the following dating app review. Does the review discuss reiewer having trouble talking to and meeting people on the app? Does the review discuss the recommended matchers being bad (ex. the app filters not working, reviewer doesn't like the matches, the reviewer not finding any matches, etc)? Simply stating 'good' in the review without further detail does not qualify for this 'bad match' code. Stating the app has bugs and has technical difficulties does not qualify for this 'bad match' code. Answer only with a number: 1 if the review discusses the app provides bad potential matches for the reviewer, 0 if the review does not discuss the app providing bad potential matches for the reviewer.\"\n",
    "\n",
    "pos_sent_question = \"Read the following dating app review. Does the review reflect positive sentiment from the user? Answer only with a number: 1 if the review reflects positive sentiment, 0 if the review does not reflect positive sentiment.\"\n",
    "\n",
    "neg_sent_question = \"Read the following dating app review. Does the review reflect negative sentiment from the user? Answer only with a number: 1 if the review reflects negative sentiment, 0 if the review does not reflect negative sentiment.\"\n",
    "\n",
    "neut_sent_question = \"Read the following dating app review. Does the review reflect neutral sentiment from the user? Answer only with a number: 1 if the review reflects neutral sentiment, 0 if the review does not reflect neutral sentiment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# write function for GPT to use above prompts to code review text\n",
    "\n",
    "def process_chatgpt(data, question_variable, output_variable):\n",
    "    for i in range(len(data)):\n",
    "        print(i)\n",
    "        question = data[question_variable][i]\n",
    "        text = data.iloc[i, 3]\n",
    "        concat = question + \" \" + text\n",
    "        result = hey_chatGPT(concat)\n",
    "        while not result:\n",
    "            result = hey_chatGPT(concat)\n",
    "            print(result)\n",
    "        print(result)\n",
    "        data.at[i, output_variable] = result\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# apply above function for GPT to code review text and insert codes in the created _gpt columns\n",
    "\n",
    "data = process_chatgpt(data, 'emojis_question', 'emojis_gpt')\n",
    "data = process_chatgpt(data, 'pos_ux_question', 'pos_ux_gpt')\n",
    "data = process_chatgpt(data, 'neg_ux_question', 'neg_ux_gpt')\n",
    "data = process_chatgpt(data, 'pos_ppl_question', 'pos_ppl_gpt')\n",
    "data = process_chatgpt(data, 'neg_ppl_question', 'neg_ppl_gpt')\n",
    "data = process_chatgpt(data, 'price_question', 'price_gpt')\n",
    "data = process_chatgpt(data, 'fraud_question', 'fraud_gpt')\n",
    "data = process_chatgpt(data, 'ban_question', 'ban_gpt')\n",
    "data = process_chatgpt(data, 'good_match_question', 'good_match_gpt')\n",
    "data = process_chatgpt(data, 'bad_match_question', 'bad_match_gpt')\n",
    "data = process_chatgpt(data, 'pos_sent_question', 'pos_sent_gpt')\n",
    "data = process_chatgpt(data, 'neg_sent_question', 'neg_sent_gpt')\n",
    "data = process_chatgpt(data, 'neut_sent_question', 'neut_sent_gpt')\n",
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pre process data\n",
    "\n",
    "cols = [\"emojis_gpt\", \"pos_ux_gpt\", \"neg_ux_gpt\", \"pos_ppl_gpt\", \"neg_ppl_gpt\", \"price_gpt\", \"fraud_gpt\", \"ban_gpt\", \"good_match_gpt\", \"bad_match_gpt\", \"pos_sent_gpt\", \"neg_sent_gpt\", \"neut_sent_gpt\"]\n",
    "og_cols = [\"emojis\", \"pos_ux\", \"neg_ux\", \"pos_ppl\", \"neg_ppl\", \"price\", \"fraud\", \"ban\", \"good_match\", \"bad_match\", \"pos_sent\", \"neg_sent\", \"neut_sent\"]\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = data[col].str[:1]\n",
    "\n",
    "for og_col in og_cols:\n",
    "    data[og_col] = data[og_col].str[:1]\n",
    "\n",
    "for col in cols:\n",
    "    data[col] = pd.to_numeric(data[col])\n",
    "\n",
    "for og_col in og_cols:\n",
    "    data[og_col] = pd.to_numeric(data[og_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# run correlation table to test interrater reliability between training data and GPT\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define a list of variable pairs\n",
    "pairs = {\n",
    "    \"emojis\": [\"emojis\", \"emojis_gpt\"],\n",
    "    \"pos_ux\": [\"pos_ux\", \"pos_ux_gpt\"],\n",
    "    \"neg_ux\": [\"neg_ux\", \"neg_ux_gpt\"],\n",
    "    \"price\": [\"price\", \"price_gpt\"],\n",
    "    \"fraud\": [\"fraud\", \"fraud_gpt\"],\n",
    "    \"ban\": [\"ban\", \"ban_gpt\"],\n",
    "    \"good_match\": [\"good_match\", \"good_match_gpt\"],\n",
    "    \"bad_match\": [\"bad_match\", \"bad_match_gpt\"],\n",
    "    \"pos_sent\": [\"pos_sent\", \"pos_sent_gpt\"],\n",
    "    \"neg_sent\": [\"neg_sent\", \"neg_sent_gpt\"],\n",
    "    \"neut_sent\": [\"neut_sent\", \"neut_sent_gpt\"]\n",
    "}\n",
    "\n",
    "# Create an empty data frame to store results\n",
    "corr_results = pd.DataFrame(columns=[\"variable\", \"correlation\"])\n",
    "\n",
    "# Loop over each pair, run the correlation test, and extract the relevant info\n",
    "for var, cols in pairs.items():\n",
    "    col1, col2 = cols\n",
    "\n",
    "    # Run the correlation test\n",
    "    test_result = pearsonr(data[col1], data[col2])\n",
    "\n",
    "    # Append results to the data frame\n",
    "    corr_results = corr_results.append({\"variable\": var, \"correlation\": test_result[0]}, ignore_index=True)\n",
    "\n",
    "# Print the results\n",
    "print(corr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# run kappa values to test interrater reliability between training data and GPT\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.stats.inter_rater import kappa2\n",
    "\n",
    "# Create an empty data frame to store the results\n",
    "kappa_results = pd.DataFrame(columns=['variable', 'kappa'])\n",
    "\n",
    "# Loop over each pair, calculate kappa2, and extract the value and p-value\n",
    "for var in pairs:\n",
    "    test_result = kappa2(data[pairs[var]], weight='unweighted')\n",
    "    kappa_val = test_result.value\n",
    "    kappa_results = kappa_results.append({'variable': var, 'kappa': kappa_val}, ignore_index=True)\n",
    "\n",
    "# Print the results table\n",
    "print(kappa_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pull specific codes out and see where GPT and trianing data have mismatches. Can repeat for every code.\n",
    "\n",
    "from kaleido.scopes.plotly import PlotlyScope\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data[data['pos_ux'] != data['pos_ux_gpt']]\n",
    "data = data[['content', 'pos_ux', 'pos_ux_gpt']]\n",
    "data.to_html('output.html', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then repeat this process for the dataset we want to analyze, just by swapping out the dataframe and removing the interrater reliability checking steps!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
